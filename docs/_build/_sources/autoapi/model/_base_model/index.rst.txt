:py:mod:`model._base_model`
===========================

.. py:module:: model._base_model


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   model._base_model.BaseModel




.. py:class:: BaseModel(task, optuna_trial, encoding = None, n_outputs = 1)

   Bases: :py:obj:`abc.ABC`

   BaseModel parent class for all models that can be used within the framework.
   Every model must be based on BaseModel directly or BaseModel's child classes, e.g. SklearnModel or TorchModel

   ## Attributes ##
       # Class attributes #
       standard_encoding: str : the standard encoding for this model
       possible_encodings: List<str> : a list of all encodings that are possible according to the model definition

       # Instance attributes #
       task: str : ML task (regression or classification) depending on target variable
       optuna_trial: optuna.trial.Trial : trial of optuna for optimization
       encoding: str : the encoding to use (standard encoding or user-defined)
       all_hyperparams: dict : dictionary with all hyperparameters with related info that can be tuned
                               (structure see define_hyperparams_to_tune())
       model: model object

   .. py:method:: standard_encoding(cls)
      :property:

      standard_encoding: the standard encoding for this model


   .. py:method:: possible_encodings(cls)
      :property:

      possible_encodings: a list of all encodings that are possible according to the model definition


   .. py:method:: define_model(self)
      :abstractmethod:

      Method that defines the model that needs to be optimized.
      Hyperparams to tune have to be specified in all_hyperparams and suggested via suggest_hyperparam_to_optuna().
      The hyperparameters have to be included directly in the model definiton to be optimized.
          e.g. if you want to optimize the number of layers, do something like
              n_layers = self.suggest_hyperparam_to_optuna('n_layers') # same name in define_hyperparams_to_tune()
              for layer in n_layers:
                  do something
          Then the number of layers will be optimized by optuna.


   .. py:method:: define_hyperparams_to_tune(self)
      :abstractmethod:

      Method that defines the hyperparameters that should be tuned during optimization and their ranges.
      Required format is a dictionary with:
          {
              'name_hyperparam_1':
                  {
                  # MANDATORY ITEMS
                  'datatype': 'float' | 'int' | 'categorical',
                  FOR DATATYPE 'categorical':
                      'list_of_values': []  # List of all possible values
                  FOR DATATYPE in ['float', 'int']:
                      'lower_bound': value_lower_bound,
                      'upper_bound': value_upper_bound,
                      # OPTIONAL ITEMS (only for ['float', 'int']):
                      'log': True | False  # sample value from log domain or not
                      'step': step_size # step of discretization.
                                          # Caution: cannot be combined with log=True
                                                          - in case of 'float' in general and
                                                          - for step!=1 in case of 'int'
                  },
              'name_hyperparam_2':
                  {
                  ...
                  },
              ...
              'name_hyperparam_k':
                  {
                  ...
                  }
          }
      If you want to use a similar hyperparameter multiple times (e.g. Dropout after several layers),
      you only need to specify the hyperparameter once. Individual parameters for every suggestion will be created.


   .. py:method:: retrain(self, X_retrain, y_retrain)
      :abstractmethod:

      Method that runs the retraining of the model
      :param X_retrain: feature matrix for retraining
      :param y_retrain: target vector for retraining


   .. py:method:: predict(self, X_in)
      :abstractmethod:

      Method that predicts target values based on the input X_in
      :param X_in: feature matrix as input
      :return: numpy array with the predicted values


   .. py:method:: train_val_loop(self, X_train, y_train, X_val, y_val)
      :abstractmethod:

      Method that runs the whole training and validation loop
      :param X_train: feature matrix for the training
      :param y_train: target vector for training
      :param X_val: feature matrix for validation
      :param y_val: target vector for validation
      :return: predictions on validation set


   .. py:method:: suggest_hyperparam_to_optuna(self, hyperparam_name)

      Suggest a hyperparameter of hyperparam_dict to the optuna trial to optimize it.
      If you want to add a parameter to your model / in your pipeline to be optimized, you need to call this method
      :param hyperparam_name: name of the hyperparameter to be tuned (see define_hyperparams_to_tune())
      :return: suggested value


   .. py:method:: suggest_all_hyperparams_to_optuna(self)

      Some models accept a dictionary with the model parameters.
      This method suggests all hyperparameters in all_hyperparams and gives back a dictionary containing them.
      :return: dictionary with suggested hyperparameters


   .. py:method:: save_model(self, path, filename)

      Method to persist the whole model object on a hard drive (can be loaded with joblib.load(filepath))
      :param path: path where the model will be saved
      :param filename: filename of the model



